# podman build --platform linux/amd64 -f redhat-distribution/Containerfile -t rh .
FROM --platform=linux/amd64 quay.io/aipcc/base-images/cpu:2.0-1749153990
WORKDIR /opt/app-root

# If you have the llama-stack installed, dependencies can be generated with:
# CONTAINER_BINARY=podman llama stack build --config redhat-distribution/build.yaml --print-deps-only
RUN pip install --no-cache sqlalchemy opentelemetry-sdk psycopg2-binary nltk sqlalchemy[asyncio] scikit-learn scipy redis aiosqlite autoevals transformers datasets pymongo mcp pymilvus kubernetes llama_stack_provider_lmeval tqdm llama_stack_provider_trustyai_fms matplotlib sentencepiece chardet pypdf requests pillow openai pandas opentelemetry-exporter-otlp-proto-http numpy aiosqlite fastapi fire httpx uvicorn opentelemetry-sdk opentelemetry-exporter-otlp-proto-http
RUN pip install --no-cache sentence-transformers --no-deps
RUN pip install --no-cache torch torchvision --index-url https://download.pytorch.org/whl/cpu
RUN pip install --no-cache llama-stack
RUN mkdir -p /opt/app-root/.llama/providers.d /opt/app-root/.cache
COPY redhat-distribution/run.yaml /opt/app-root/run.yaml
COPY redhat-distribution/providers.d /opt/app-root/.llama/providers.d
ENTRYPOINT ["python", "-m", "llama_stack.distribution.server.server", "--config", "/opt/app-root/run.yaml"]
